<!doctype html>
<html lang="en">

<head>
 <meta charset="utf-8">

 <title>Hands-on Workshop</title>

 <meta name="description" content="Apache Kafka">
 <meta name="author" content="Kiruthika Samapathy">

 <meta name="apple-mobile-web-app-capable" content="yes" />
 <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

 <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

 <link rel="stylesheet" href="css/reveal.css">
 <link rel="stylesheet" href="css/theme/thoughtworks.css" id="theme">

 <!-- Code syntax highlighting -->
 <link rel="stylesheet" href="lib/css/zenburn.css">

 <!-- Printing and PDF exports -->
 <script>
  var link = document.createElement('link');
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
  document.getElementsByTagName('head')[0].appendChild(link);
 </script>

</head>

<body>

 <div class="reveal">

  <!-- Any section element inside of this container is displayed as a slide -->
  <div class="slides">

   <section class="title">
    <h1>Event streaming</h1>
    <h3>AWS Kinesis and Apache Kafka</h3>
    <aside class="notes">
     <ul>
      <li> publish-subscribe messaging designed as a distributed commit log </li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>What to expect?</h2>
    <br>
     <ul>
      <li class="fragment">Why should I care about this?</li><br>
      <p class="fragment">All that we can cover in 10 mins about...</p>
      <ul>
      <li class="fragment">AWS Kinesis</li>
      <li class="fragment">Apache Kafka</li><br>
      </ul>
      <li class="fragment">Thrilling short ride! (Demo time)</li><br>
      <li class="fragment"><b>Lots of discussion!!! :)</b></li>
     </ul>
   </section>

   <section>
    <h1>Unconstrainted data growth</h1>
    <br>
    <aside class="notes">
     <ul>
      <li><b>Motivation</b></li>
      <li>server logs</li>
      <li>click streams, user engagement</li>
      <li>social media - tweets</li>
      <li>sensor - weather</li>
     </ul>
    </aside>
  </section>

  <section>
   <h2>Just in 60 seconds...</h2>
   <br>
   <img width="700" data-src="images/in-60-seconds.png" alt="Kafka overview"  style="border:0;box-shadow:none"><br>
   <small style="padding-top : 10px">Image source: http://go-globe.com</small>
   <aside class="notes">
    <ul>
     <li><b>Motivation</b></li>
     <li>relational database - dump - caches - reliable but not scalable</li>
     <li>non-relational database</li>
     <li>click streams, user engagement - high through put</li>
     <li>social media - tweets</li>
     <li>sensor - weather</li>
     <li>server/application logs</li><br>
     <li> so its not just not abt data, its about getting usable/high quality data</li>
     <li> bec 90% of time spent on getting data </li>
    </ul>
   </aside>
  </section>

  <section>
   <h2>Typical data flow</h2>
   <br>
    <ul>
     <li class="fragment">Clients... producing information</li>
     <li class="fragment">...Aggregator... enabling</li>
     <li class="fragment">Continuous Processing</li>
     <li class="fragment">Storage</li>
     <li class="fragment">Analytics and Reporting</li>
    </ul>
 </section>

 <section class="title" data-background="images/data_background.jpg">
  <aside class="notes">
   <ul>
    <li> High volume of events/activities to be recorded (100k+/sec) </li>
    <li> messages/events to be delivered at least once
     <li> mix of online and batch consumers</li>
     <li> consumers at different pace</li>
     <li> Multiple consumers per message, where in a typical message queue you will be forced to use a queue per consumer</li>
     <li> there by duplicating all the data</li>
     <li> Heavy message queue affects over all throughput</li>
     <li> Mine is a data-driven company; Events like user activities drive the business and events are becoming first class citizens </li>
     <li> no real-time data processing tool is complete without Kafka integration</li>
     <br>
     <li> your message queue performance dips severly whenever it is backed up beyond what could be kept in memory </li>
   </ul>
  </aside>
 </section>

  <section>
   <h2>Can you take <br><br>every click,<br> database change, <br>application log <br><br>and expose them as <br>real-time<br> streams of data?</h2>
   <br>
  </section>

  <section>
   <h1>AWS Kinesis</h1>
    <img width="300" data-src="images/kinesis.png" alt="Kinesis" style="border:0">
   <br>
  </section>

  <section>
   <h2>AWS Kinesis - Introduction</h2>
      <ul>
      <li class="fragment">Hosted in the cloud</li>
      <li class="fragment">Highly scalable</li>
      <li class="fragment">AWS SDK with thorough documentation</li>
      <li class="fragment">Third party libraries for more data goodness</li>
     </ul>
   <br>
  </section>
    <section>
   <h1>Architecture</h1>
    <img width="1200" data-src="images/kinesis_architecture.png" alt="Kinesis" style="border:0">
   <br>
  </section>
    <section>
   <h2>Key Concepts</h2>
      <ul>
      <li class="fragment">Consumers and Producers</li>
      <li class="fragment">Streams</li>
      <li class="fragment">Shards</li>
      <li class="fragment">Data Records</li>
     </ul>
   <br>
  </section>
  <section>
     <h2>Libraries</h2>
      <ul>
      <li class="fragment">Interact with Kinesis via a RESTful API</li>
      <li class="fragment">AWS SDK</li>
      <li class="fragment">Kinesis Client Library (KCL)</li>
      <li class="fragment">Kinesis Producer Library (KPL)</li>
     </ul>
   <br>
  </section>
  <section>
   <h2>Distributed consumer apps</h2>
      insert diagram here that shows multiple instances of the same app, reading from different shards
   <br>
  </section>
  <section>
   <h2>Shards</h2>
      insert diagram here that explains shards & workers & leases
   <br>
  </section>
  <section>
   <h2>AWS Kinesis - Shards, Workers, etc</h2>

   <br>
  </section>
  <section>
   <h1>Apache Kafka</h1>
   <img width="200" data-src="images/kafka_icon.png" alt="Kafka overview"  style="border:0;box-shadow:none">
   <br>
  </section>

   <section>
    <h2>Apache Kafka - Introduction</h2>
    <br>
     <ul>
      <li class="fragment"><b>Open Source (Apache 2.0)</b></li>
      <li class="fragment">Initiated by LinkedIn SNA team</li>
      <li class="fragment">Written in Scala</li>
      <li class="fragment">Kind of data pipeline/commit log</li>
     </ul>
     <aside class="notes">
       <ul>
         <li> data eng + machine learning - OPs + Dev team - recommendation system</li>
         <li> only 14% of the data was consumed</li> <br>
         <li> had big hadoop cluster set up which is ready to do all fun stuff given data<li>
         <li> limitless number of data sources </li>
         <li> what was missing was the middle piece, which wil collect this information and hand over to analytical systems</li>
         <li> before delving into this, tried many messaging systems </li>
         <li> what we are seeing in the outside world is the third attempt (activemq, rabbitmq) within LinkedIn</li>
       </ul>
     </aside>
   </section>

   <section>
    <h2>When to use?</h2>
    <br>
     <ul>
      <ul class="fragment">
        <strong>Scalability</strong>
        <li>Highly scalable, really lots and lots of data</li>
        <li>Scalability - at an org. level</li>
      </ul><br>
      <ul class="fragment">
        <strong>Reliability</strong>
        <li>Guarantees to some extent</li>
        <li>Ability to play parallelism</li>
      </ul><br>
      <ul class="fragment">
        <strong>Distributed</strong>
        <li>Multiple producers and multiple consumers</li>
      </ul><br>
     </ul>
     <aside class="notes">
       As per JMS, each message has to be acknowledged back <br>
       Exactly one delivery guarantee requires two-phase commit. <br>
     </aside>
   </section>

   <section>
    <h2>Decoupled data pipeline</h2>
    <br>
    <img width="1400" data-src="images/kafka_overview.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
    <aside class="notes">
     <ul>
      <li> it is a quite simple abstraction</li>
      <li> Producers send messages to Kafka clusters</li>
      <li> Kafka cluster holds these messages for specified time</li>
      <li> Consumers read content from cluster on their own pace</li>
      <li> producer need not know anything abt downstream pipeline</li>
      <li> not very different from typical messaging queues in terms of usage</li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>How is Kafka different?</h2>
    <img width="800" data-src="images/simple_commit_log.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
    <aside class="notes">
     <ul>
      <li></li>
      <li> abstraction - idea from how distributed systems handle data replication</li>
      <li> sequence of records - timed old to new - each record has unique id</li>
      <li> in any company - some distributed systems </li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>Characteristics</h2>
    <br>
    <ul>
     <li class="fragment">Explicitly distributed</li>
     <li class="fragment">Partitioned</li>
     <li class="fragment">Replicated</li>
     <li class="fragment">Dumb pipelines</li>
    </ul>
    <aside class="notes">
     <ul>
      <li> central commit log and all published messages are retained for a configurable period of time </li>
      <li> <b> distributed </b> kakfa is a cluster of machines called as brokers </li>
      <li> kafka assumes that producers, brokers and consumers are all spread across multiple machines</li>
      <li> Prodcers, brokers and consumers run as a logical group with the help of Zookeeper</li>
      <br>
      <li> <b> partitioned </b> helps to scale easily </li>
      <li> <b> replicated </b> which makes the system highly available and predictable </li>
      <br>
      <li> State information as what is being consumed is part of consumer and not data pipeline</li>
      <br>
      <li> break apart your entire infrastructure</li>
      <li> all your systems can dump data into it, why because it is cheap and easy to scale and predictable</li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>Kafka dictionary</h2>
    <br>
    <ul>
     <li class="fragment">Broker - Kafka Server</li>
     <li class="fragment">Producer</li>
     <li class="fragment">Consumer</li>
     <li class="fragment">Topics - Multiple partitions - Partitions replicated </li>
     <li class="fragment">Broker leader - Broker followers </li>
    </ul>
    <aside class="notes">
     <ul>
      <li> leader - current broker in-charge of the partition</li>
      <li> all producers to the given partition talk to the broker leader</li>
      <li> replication - broker consumes from leader</li>
      <li> list of broker replicas - that are upto date with the leader</li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>Brokers</h2>
    <img width="900" data-src="images/topics_partitions_overview1.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
    <aside class="notes">
     <ul>
      <li> topic - chunk of data </li>
      <li> topic is the virtual category/feed, it is the key abstraction</li>
      <li> For each topic, kafka maintains partitioned log</li>
      <li> Kafka topic is a append-only or write-ahead log</li>
      <li> so ordered is guaranteed at the partition level</li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>Replicated</h2>
    <img width="900" data-src="images/topics_partitions_overview2.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
    <aside class="notes">
     <ul>
      <li> fault tolerance </li>
      <li> Kafka controller - takes care of placements </li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>Partition leader</h2>
    <img width="900" data-src="images/topics_partitions_overview3.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
    <aside class="notes">
     <ul>
      <li> Each partition has one broker which acts as the "leader" and zero or more servers which act as “followers”, which replicates the leader </li>
      <li> If the leader fails, one of the followers will automatically become the new leader. </li>
      <li> Each server acts as a leader for some of its partitions and a follower for others so load is well balanced within the cluster. </li>
      <li> producer sends data directly to the broker that is that is the partition leader </li>
      <li> so how will the producer know that? all kafka nodes will return metadata saying which servers are alive, partition leaders info </li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>In detail...</h2>
    <img width="1200" data-src="images/replication_explanation.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
    <aside class="notes">
     <ul>
      <li> depends on use case - how to distribute/broadcast data to multiple systems</li>
      <li> central MySql database - one partition</li>
      <li> MongoDB - in turn has partitions - can map to it - depends on parallelism</li><br>
      <li> partitions are log files on the disk</li>
      <li> partitioning allow the log to scale beyond a size that will fit on a single server</li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>Overview</h2>
    <img width="800" data-src="images/topics_partitions_overview5.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
    <aside class="notes">
     <ul>
      <li> So, is kafka a queueing system or publsh-subscribe/broadcast system? </li>
      <li> it has single consumer abstraction </li>
      <li> Consumers label themselves with a consumer group name, and each message published to a topic is delivered to one consumer instance within each subscribing consumer group.</li>
      <li> If all the consumer instances have the same consumer group, then this works just like a traditional queue balancing load over the consumers.</li>
      <li> If all the consumer instances have different consumer groups, then this works like publish-subscribe and all messages are broadcast to all consumers.</li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>Topic and Partitions</h2>
    <img width="1000" data-src="images/topics_partitions_detail.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
    <aside class="notes">
     <ul>
      <li></li>
      <li> Every record is a key-value pair, key determines the partition</li>
      <li> key could be computed randomly/producer might specify the key explicitly</li>
      <li> every record has an offset number - consumer uses this offset to determine its position</li>
      <li> Messages are simply byte arrays and the developers can use them to store any object in any format – with String, JSON, and Avro the most common</li>
      <li> consumer specifies offset in the log with each request and receives back a chunk of log beginning from that position - better control and can can re-consume. </li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>Topic and Partitions</h2>
    <img width="800" data-src="images/commit_log.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
    <aside class="notes">
     <ul>
      <li> <b> Producers </b> </li>
      <li> lots of other customization options</li>
      <li>Asynchronous/Synchronous send</li>
      <li>Batching</li>
      <li>Compression</li>
      <li> <b> consumers </b> </li>
      <li> How is it different from other messaging queues? In a typical messaging system, queue will push messages to consumers and maintain all other associated metadata</li>
      <li> Each consumer process belongs to a consumer group</li>
      <li> each message is delivered to exactly one process within every consumer group</li>
      <li> In an ideal world, there will be multiple logical consumer groups, each consisting of a cluster of consuming machines</li>
      <li> In the case of large data that no matter how many consumers a topic has, a message is stored only a single time.</li>
      <li> Here, consumers pull messsages</li>
     </ul>
    </aside>
   </section>

  <section data-background="images/usecases_wordcloud.jpg">
   <h2>Use cases</h2>
   <br>
   <ul>
    <li>Real time event/log aggregations</li>
    <li>Speed layer in the Lambda architecture</li>
    <li>Real time news feeds/metrics/alerts/monitoring</li>
    <li>Data loading for data processing systems</li>
    <li>Event sourcing</li>
    <li>Commit logs</li>
   </ul>
   <aside class="notes">
    <ul>
     <li> no real-time data processing tool is complete without Kafka integration</li>
     <li> best suited when multiple consumers, as that is what it is best optimized for</li>
     <li> commit logs - database state capture</li>
     <br>
     <li> Because Kafka topics are very cheap from a performance and overhead standpoint,</li>
     <li> it’s possible for us to create as many queues as we want, scaled to the performance we want</li>
     <li> and optimizing resource utilization across the system. Because they can be created dynamically,</li>
     <li> we can make our business rules very flexible.</li>
     <li> <b> event sourcing </b> style of application design where state changes are logged as a time-ordered sequence of records </li> <br>

     <li> which is fast becoming the centre of gravity for data logging </li>
     <li> all your systems can dump data into it, why because it is cheap and easy to scale and predictable</li>
    </ul>
   </aside>
  </section>

  <section>
   <h1>Enable Stream processing</h1>
   <aside class="notes">
    infra for request-response and batch <br>
    nothing for stream <br>
    size of stream depends on what we do <br>
    main challenge is how to handle the state in the stream processing <br>
    again use commit logs - as back up for fault tolerance <br>
   </aside>
 </section>

   <section>
    <h1>Event Driven Architecture</h1>
    <aside class="notes">
      totally different ball game <br>
      example - conf registration <br>
      AWS Lambda - short lived services - containers <br>
    </aside>
  </section>

   <section>
    <h2>yes/no</h2>
    <br>
    <img width="800" data-src="images/pros_cons.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
    <aside class="notes">
     <ul>
      <b> pros </b>
      <li> Handles peak bandwidth - 20Gbps</li>
      <li> Dynamically configurable - easy to add more brokers/topics/consumers </li>
      <li> All topics are available for reading by any number of subscribers, and additional subscribers have very low overhead. </li>
      <li> Multiple clients can process data in parallel and its own pace </li>
      <li> Any system can hook to it and consume data with minimal integration work </li>
      <li> Both broker and consumer group membership is fully dynamic, which allows us to dynamically add nodes to the groups and have load automatically rebalanced without any static cluster configuration. </li>
      <li> move the cleanliness upstream </li>
      <li> thousands of terrabytes of data</li>
      <li> millions of requests per second</li>
      <br>
      <b> cons </b>
      <li> <b> zookeeper </b> Kafka will not work without zookeeper - CAP theorem CP system - unless a quorum of nodes are up (2 of 3, or 3 of 5), the whole system is unavailable</li>
      <li> Kafka 0.8 requires that the client have access to the same ZooKeeper instance as the server</li>
      <li> <b> Java clients </b> Non-java based clients are not as great as Java ones, especially with Kafka 0.8</li>
     </ul>
    </aside>
   </section>

   <section>
    <h2>Eco system</h2>
    <br>
    <img width="1500" data-src="images/kafka_plus.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
    <aside class="notes">
     <ul>
       <li> stream processing not a niche thing - bec we get data in large chunk, we think abt processing them in large chunk - debatable </li>
       <li> unix piping - pipe is kafka + code is urs </li>
       <li> kafka is like file system in Hadoop and other tools like map reduce </li>
      <li> Spark streming - </li>
      <li> Storm - on top of one of these solutions to add computation, filtering, querying, on your streams.</li>
      <li> Cassandra - as a queryable cache</li>
      <li> Samza - built to work natively with Kafka</li>
      <li> Flink - </li>
      <li> Kafka streams - </li>
     </ul>
    </aside>
   </section>

   <section>
    <br>
    <h2>Can turn this....</h2><br>
    <img width="1300" data-src="images/linkedin_before.png" alt="Kafka overview"  style="border:0;box-shadow:none">
   </section>

   <section>
    <br>
    <h2>.... into this</h2>
    <img width="900" data-src="images/linkedin_after.png" alt="Kafka overview"  style="border:0;box-shadow:none">
   </section>

   <section>
    <br>
    <h1>Kinesis = Kafka-as-a-service</h1>
   </section>

   <section>
    <h2>Why Kinesis = Kafka-as-a-service?</h2>
    <br>
    <ul>
     <li class="fragment">Fully (auto)managed</li>
     <li class="fragment">Durability gurantees</li>
     <li class="fragment">Stream (==Topic)</li>
     <li class="fragment">Shrad (==Partition) </li>
     <li class="fragment">Only REST API ( vs Low level API) </li>
     <li class="fragment">..... but slow readers (maximum of 5/per shrad) </li>
    </ul>
   </section>

   <section>
    <h1>Demo</h1>
    <br>
   </section>

   <section>
    <h2>Demo Overview</h2>
    <img width="1200" data-src="images/demo_explanation.jpg" alt="Kafka overview"  style="border:0;box-shadow:none">
  </section>

  <section>
   <h1>Demo in action</h1>
 </section>
   <!-- END OF TUTORIAL SLIDES -->

  </div>

 </div>

 <script src="lib/js/head.min.js"></script>
 <script src="js/reveal.js"></script>

 <script>
  // Full list of configuration options available at:
  // https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
   controls: true,
   progress: true,
   history: true,
   center: true,
   hideAddressBar: true,

   transition: 'slide', // none/fade/slide/convex/concave/zoom

   // Optional reveal.js plugins
   dependencies: [{
    src: 'lib/js/classList.js',
    condition: function() {
     return !document.body.classList;
    }
   }, {
    src: 'plugin/markdown/marked.js',
    condition: function() {
     return !!document.querySelector('[data-markdown]');
    }
   }, {
    src: 'plugin/markdown/markdown.js',
    condition: function() {
     return !!document.querySelector('[data-markdown]');
    }
   }, {
    src: 'plugin/highlight/highlight.js',
    async: true,
    condition: function() {
     return !!document.querySelector('pre code');
    },
    callback: function() {
     hljs.initHighlightingOnLoad();
    }
   }, {
    src: 'plugin/zoom-js/zoom.js',
    async: true
   }, {
    src: 'plugin/notes/notes.js',
    async: true
   }]
  });
 </script>

</body>

</html>
